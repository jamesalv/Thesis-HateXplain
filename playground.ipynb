{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e336e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\envs\\hatexplain-clone\\Lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\envs\\hatexplain-clone\\Lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from dataCollector import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4daa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_params('Params/bert_attention_TRUE.json')\n",
    "params['path'] = 'Data/dataset.json'\n",
    "params['num_classes'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658659da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: {'alpha': 0.5, 'att_lambda': 0.001, 'attention': 'N/A', 'auto_weights': True, 'batch_size': 16, 'bert_tokens': True, 'decay': False, 'device': 'cuda', 'drop_embed': 'N/A', 'drop_fc': 'N/A', 'drop_hidden': 'N/A', 'dropout_bert': 0.1, 'embed_size': 'N/A', 'embeddings': 'N/A', 'epochs': 20, 'epsilon': 1e-08, 'hidden_size': 'N/A', 'include_special': False, 'is_model': True, 'learning_rate': 2e-05, 'logging': 'local', 'majority': 2, 'max_length': 128, 'method': 'additive', 'model_name': 'birnn', 'normalized': False, 'not_recollect': True, 'num_classes': 3, 'num_supervised_heads': 6, 'p_value': 0.8, 'padding_idx': 'N/A', 'path_files': 'bert-base-uncased', 'random_seed': 42.0, 'save_only_bert': False, 'seq_model': 'N/A', 'set_decay': 0.1, 'supervised_layer_pos': 11, 'to_save': False, 'train_att': True, 'train_embed': 'N/A', 'type_attention': 'softmax', 'variance': 5, 'vocab_size': 'N/A', 'weights': '[1.0795518  0.82139814 1.1678787 ]', 'what_bert': 'weighted', 'window': 4}\n",
      "Using GPU: device\n",
      "Preprocessing data from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 20148/20148 [00:37<00:00, 542.51entry/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data: 20148\n",
      "Uncertain data: 919\n",
      "Total final data count: 19229\n",
      "Saving preprocessed data to cache: cache\\preprocessed_data_-935987485734774957.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Weighted_BERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "e:\\Campus\\FinalProjectv2\\utils.py:50: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  cr_ent += cross_entropy(input1[h][mask[h]], target[h][mask[h]])\n",
      "c:\\Users\\USER\\miniconda3\\envs\\hatexplain-clone\\Lib\\site-packages\\torch\\autograd\\graph.py:823: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:/actions-runner/_work/pytorch/pytorch/pytorch/aten/src\\ATen/native/IndexingUtils.h:30.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "962it [09:03,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.9471992133127181\n",
      "model previously passed\n",
      "Running eval on  train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255it [02:33,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mParams/bert_attention_TRUE.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Campus\\FinalProjectv2\\training.py:394\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPU not available, using CPU instead !\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    392\u001b[39m     device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Campus\\FinalProjectv2\\training.py:302\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(params, device)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Store the loss value for plotting the learning curve.\u001b[39;00m\n\u001b[32m    294\u001b[39m loss_values.append(avg_train_loss)\n\u001b[32m    295\u001b[39m (\n\u001b[32m    296\u001b[39m     train_fscore,\n\u001b[32m    297\u001b[39m     train_accuracy,\n\u001b[32m    298\u001b[39m     train_precision,\n\u001b[32m    299\u001b[39m     train_recall,\n\u001b[32m    300\u001b[39m     train_roc_auc,\n\u001b[32m    301\u001b[39m     _,\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m ) = \u001b[43meval_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m val_fscore, val_accuracy, val_precision, val_recall, val_roc_auc, _ = (\n\u001b[32m    304\u001b[39m     eval_phase(params, \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m, model, val_dataloader, device)\n\u001b[32m    305\u001b[39m )\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     test_fscore,\n\u001b[32m    308\u001b[39m     test_accuracy,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     logits_all_final,\n\u001b[32m    313\u001b[39m ) = eval_phase(params, \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, model, test_dataloader, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Campus\\FinalProjectv2\\training.py:94\u001b[39m, in \u001b[36meval_phase\u001b[39m\u001b[34m(params, which_files, model, test_dataloader, device)\u001b[39m\n\u001b[32m     92\u001b[39m logits = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Move logits and labels to CPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m logits = \u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     95\u001b[39m label_ids = b_labels.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m).numpy()\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Calculate the accuracy for this batch of test sentences.\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Accumulate the total accuracy.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(path='Params/bert_attention_TRUE.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatexplain-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
